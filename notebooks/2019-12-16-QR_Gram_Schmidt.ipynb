{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR分解(经典Gram-Schmidt算法实现)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要介绍QR分解的一种实现算法：经典Gram-Schmidt算法，并且以4x3矩阵$A$为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = [v_1, v_2, v_3] = \\left[\\begin{array}{ccc}{1} & {0} & {1} \\\\ {2} & {0} & {0} \\\\ {0} & {1} & {0} \\\\ {1} & {-1} & {1}\\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.],\n",
       "       [ 2.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1., -1.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0, 1],\n",
    "              [2, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [1, -1, 1]], dtype=float)\n",
    "A  # 4 x 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经典Gram-Schmidt算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典Gram-Schmidt算法和Gram-Schmidt正交化相关，基本思想是将还未正交化的向量扣除其在已经正交化的向量上的投影，再对其单位化。以矩阵$A$为例:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步**：$v_1$除上它的长度就得到$q_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: [1. 2. 0. 1.]\n",
      "q1: [0.40824829 0.81649658 0.         0.40824829]\n",
      "length of q1: 1.0\n"
     ]
    }
   ],
   "source": [
    "v1 = A[:, 0]\n",
    "q1 = v1 / np.linalg.norm(v1)\n",
    "print(\"v1:\", v1)\n",
    "print(\"q1:\", q1)\n",
    "print(\"length of q1:\", np.linalg.norm(q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步**：$v_2$扣除其在$q_1$上的投影，再单位化得到$q_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2: [ 0.  0.  1. -1.]\n",
      "q2: [ 0.12309149  0.24618298  0.73854895 -0.61545745]\n",
      "length of q2: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "v2 = A[:, 1]\n",
    "v2\n",
    "q2 = (v2 - (q1.T @ v2) * q1)  / np.linalg.norm(v2 - (q1.T @ v2) * q1)\n",
    "print(\"v2:\", v2)\n",
    "print(\"q2:\", q2)\n",
    "print(\"length of q2:\", np.linalg.norm(q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步**：$v_3$扣除其在$q_1$、$q_2$上的投影，再单位化得到$q_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3: [1. 0. 0. 1.]\n",
      "q3: [ 0.69631062 -0.52223297  0.34815531  0.34815531]\n",
      "length of q3: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "v3 = A[:, 2]\n",
    "q3 = (v3 - (q1.T @ v3) * q1 - (q2.T @ v3) * q2) / np.linalg.norm(v3 - (q1.T @ v3) * q1 - (q2.T @ v3) * q2)\n",
    "print(\"v3:\", v3)\n",
    "print(\"q3:\", q3)\n",
    "print(\"length of q3:\", np.linalg.norm(q3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就得到列正交的$Q$矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829,  0.12309149,  0.69631062],\n",
       "       [ 0.81649658,  0.24618298, -0.52223297],\n",
       "       [ 0.        ,  0.73854895,  0.34815531],\n",
       "       [ 0.40824829, -0.61545745,  0.34815531]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([q1, q2, q3]).T\n",
    "Q # 4 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上三角矩阵$R$，它的对角线元素为$v$扣除了在前面所有的$q$上的投影之后剩下的长度，而非对角线元素为$v$在前面的各个$q$上的投影量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.44948974, -0.40824829,  0.81649658],\n",
       "       [ 0.        ,  1.3540064 , -0.49236596],\n",
       "       [ 0.        ,  0.        ,  1.04446594]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r11 = np.linalg.norm(v1)\n",
    "\n",
    "r12 = q1.T @ v2\n",
    "r22 = np.linalg.norm(v2 - (q1.T @ v2) * q1)\n",
    "\n",
    "r13 = q1.T @ v3\n",
    "r23 = q2.T @ v3\n",
    "r33 = np.linalg.norm(v3 - (q1.T @ v3) * q1 - (q2.T @ v3) * q2)\n",
    "\n",
    "R = np.array([\n",
    "    [r11, r12, r13],\n",
    "    [  0, r22, r23],\n",
    "    [  0,   0, r33]\n",
    "])\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检验一下$QR$与$A$是否相等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(Q @ R, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再和`numpy.linalg`的结果比较一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.40824829, -0.12309149, -0.69631062],\n",
       "        [-0.81649658, -0.24618298,  0.52223297],\n",
       "        [-0.        , -0.73854895, -0.34815531],\n",
       "        [-0.40824829,  0.61545745, -0.34815531]]),\n",
       " array([[-2.44948974,  0.40824829, -0.81649658],\n",
       "        [ 0.        , -1.3540064 ,  0.49236596],\n",
       "        [ 0.        ,  0.        , -1.04446594]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乍一看好像不一样但不要担心，只是$Q$和$R$都多了一个负号，相乘之后的结果仍然是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以证明，对于列满秩的矩阵$A$，如果我们规定$R$的对角线元素为正，则QR分解式是唯一的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "热身结束，来看看代码怎么写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGS(A):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: (m x n) matrix with n linearly independent columns\n",
    "    Returns:\n",
    "        Q: (m x n) matrix with n orthonormal columns\n",
    "        R: (n x n) upper triangular matrix\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    \n",
    "    Q = np.zeros(shape=(m, n))\n",
    "    R = np.zeros(shape=(n, n))\n",
    "    # 遍历A的列\n",
    "    for j in range(n):\n",
    "        vj = A[:, j]\n",
    "        for i in range(j):\n",
    "            R[i, j] = Q[:, i].T @ A[:, j]  # 在前面q上的投影量\n",
    "            vj = vj - R[i, j] * Q[:, i]    # 不断扣除在前面的q上的投影\n",
    "        R[j, j] = np.linalg.norm(vj)\n",
    "        Q[:, j] = vj / R[j, j]             # 标准化\n",
    "        \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.40824829,  0.12309149,  0.69631062],\n",
       "        [ 0.81649658,  0.24618298, -0.52223297],\n",
       "        [ 0.        ,  0.73854895,  0.34815531],\n",
       "        [ 0.40824829, -0.61545745,  0.34815531]]),\n",
       " array([[ 2.44948974, -0.40824829,  0.81649658],\n",
       "        [ 0.        ,  1.3540064 , -0.49236596],\n",
       "        [ 0.        ,  0.        ,  1.04446594]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CGS(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典Gram-Schmidt算法还是比较直观的，给定向量$v$，不断扣除其在前面所有的$q$上的投影，扣完以后$v$就和它们正交了，只需再单位化即可，通过这种方式一个一个地造标准正交化向量。\n",
    "\n",
    "而接下来所要谈的改良版Gram-Schmidt算法和它的区别是：给定向量$v$，改良版Gram-Schmidt先标准化得到$q$，然后将后面每一个还未正交化的向量都扣除其在$q$上的投影，这样它们就都和$v$正交了。通过这样的方式，**每一次迭代后面的向量就会和当前的向量正交，而经典Gram-Schmidt算法是当前的向量和之前所有的向量正交**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_daily",
   "language": "python",
   "name": "th"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
