{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundations Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Question 1-2 are about linear regression.\n",
    "\n",
    "Consider a noisy target $y=\\mathbf{w}_{f}^\\top \\mathbf{x}+\\epsilon$, where $\\mathbf{x} \\in \\mathbb{R}^{d}$ (with the added coordinate $x_0 = 1$, $y\\in \\mathbb{R}$, $\\mathbf{w}_{f}$ is an unknown vector, and $\\epsilon$ is a noise term with zero mean and $\\sigma^2$ variance. Assume $\\epsilon$ is independent of $\\mathbf{x}$ and all other $\\epsilon$'s. If linear regression is carried out using a training data set $\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_{1}, y_{1}\\right), \\ldots,\\left(\\mathbf{x}_{N}, y_{N}\\right)\\right\\}$, and outputs the parameter vector $\\mathbf{w}_{\\text{lin}}$, it can be shown that the expected in-sample error $E_{\\text{in}}$ with respect to $\\mathcal{D}$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_{\\mathcal{D}}\\left[E_{\\text {in }}\\left(\\mathbf{w}_{\\text {lin }}\\right)\\right]=\\sigma^{2}\\left(1-\\frac{d+1}{N}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\sigma = 0.1$ and $d=8$, which among the following choices is the smallest number of example $N$ that will result in an expected $E_{\\text{in}}$ greater than 0.008?\n",
    "\n",
    "- 500\n",
    "- 25\n",
    "- 100\n",
    "- 1000\n",
    "- 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "d = 8\n",
    "\n",
    "expected_E_in = lambda N: sigma**2 * (1 - (d + 1) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500: 0.009820000000000002\n",
      "  25: 0.006400000000000001\n",
      " 100: 0.009100000000000002\n",
      "1000: 0.009910000000000002\n",
      "  10: 0.001\n"
     ]
    }
   ],
   "source": [
    "for N in [500, 25, 100, 1000, 10]:\n",
    "    print(f\"{N:>4}: {expected_E_in(N)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the answer is $N = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Recall that we have introduced the hat matrix $\\mathrm{H}=\\mathrm{X}\\left(\\mathrm{X}^\\top \\mathrm{X}\\right)^{-1} \\mathrm{X}^\\top$ in class, where $\\mathbf{X} \\in \\mathbb{R}^{N \\times(d+1)}$ for $N$ examples and $d$ features. Assume $\\mathrm{X}^\\top \\mathrm{X}$ is invertible, which statements of $\\mathrm{H}$ are true?\n",
    "\n",
    "- (a) $\\mathrm{H}$ is always positive semi-definite.\n",
    "- (b) $\\mathrm{H}$ is always invertible.\n",
    "- (c) some eigenvalues of $\\mathrm{H}$ are possibly bigger than 1\n",
    "- (d) $d+1$ eigenvalues of $\\mathrm{H}$ are exactly 1.\n",
    "- (e) $\\mathrm{H}^{1126} = \\mathrm{H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "- (a): true, since the eigenvalues of $\\mathrm{H}$ are all non-negative.\n",
    "- (b): false, since the rank of $\\mathrm{H}$ is $d+1$, which may be less than $N$.\n",
    "- (c): false.\n",
    "- (d): true.\n",
    "- (e): true, since $\\mathrm{H}$ is an orthogonal projection matrix, project twice does not have much more effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Question 3-5 are about error and SGD\n",
    "\n",
    "Which of the following is an upper bound of $\\left[\\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{x}\\right) \\neq y\\right]$ for $y \\in \\{-1, +1\\}$?\n",
    "\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\left(\\max \\left(0,1-y \\mathbf{w}^{T} \\mathbf{x}\\right)\\right)^{2}$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\max \\left(0,-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\theta\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "If we draw the picture, than we can find that only the second error function $\\operatorname{err}(\\mathbf{w})=\\left(\\max \\left(0,1-y \\mathbf{w}^{T} \\mathbf{x}\\right)\\right)^{2}$ is an upper bound for the 0-1 error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Which of the following is not a everywhere-differentiable function of $\\mathbf{w}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\operatorname{err}(\\mathbf{w})=\\left(\\max \\left(0,1-y \\mathbf{w}^{T} \\mathbf{x}\\right)\\right)^{2}$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\max \\left(0,-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\theta\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "The error function $\\operatorname{err}(\\mathbf{w})=\\max \\left(0,-y \\mathbf{w}^{T} \\mathbf{x}\\right)$ is not differentiable at $\\mathbf{w}$ where $\\mathbf{w}^T\\mathbf{x} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "When using SGD on the following error functions and 'ignoring' some singular points that are not differentiable, which of the following error function results in PLA?\n",
    "\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\left(\\max \\left(0,1-y \\mathbf{w}^{T} \\mathbf{x}\\right)\\right)^{2}$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\max \\left(0,-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\theta\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$\n",
    "- $\\operatorname{err}(\\mathbf{w})=\\left(-y \\mathbf{w}^{T} \\mathbf{x}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:** The answer is $\\operatorname{err}(\\mathbf{w})=\\max \\left(0,-y \\mathbf{w}^{T} \\mathbf{x}\\right)$.\n",
    "\n",
    "At $\\mathbf{w}^{T} \\mathbf{x} \\neq 0 $, this error function is differentiable. \n",
    "\n",
    "If $y = -1$, then\n",
    "$$\n",
    "\\nabla\\text{err}(\\mathbf{w})=\\left\\{\\begin{array}{ll}\n",
    "\\mathbf{0}, & \\mathbf{w}^T\\mathbf{x} <0 \\\\\n",
    "\\mathbf{x}, & \\mathbf{w}^T\\mathbf{x} >0\n",
    "\\end{array}\\right..\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y = 1$, then\n",
    "\n",
    "$$\n",
    "\\nabla\\text{err}(\\mathbf{w})=\\left\\{\\begin{array}{ll}\n",
    "\\mathbf{0}, & \\mathbf{w}^T\\mathbf{x} >0 \\\\\n",
    "\\mathbf{-x}, & \\mathbf{w}^T\\mathbf{x} <0\n",
    "\\end{array}\\right..\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the two cases together as \n",
    "\n",
    "$$\n",
    "\\nabla\\text{err}(\\mathbf{w})=\\left\\{\\begin{array}{ll}\n",
    "\\mathbf{0}, & y\\mathbf{w}^T\\mathbf{x} >0 \\\\\n",
    "-y\\mathbf{x}, & y\\mathbf{w}^T\\mathbf{x} <0\n",
    "\\end{array}\\right..\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in PLA, we update $\\mathbf{w}$ only at a misclassified point, i.e., $y \\mathbf{w}^T\\mathbf{x} < 0$, and the update value is exactly $y \\mathbf{x}$, which is the opposite direction of its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "For Question 6-10, you will play with gradient descent algorithm and variants. Consider a function\n",
    "\n",
    "$$E(u, v)=e^{u}+e^{2 v}+e^{u v}+u^{2}-2 u v+2 v^{2}-3 u-2 v.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the gradient $\\nabla E(u, v)$ around $(u, v)=(0,0)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "The gradient of $E(u, v)$ is \n",
    "\n",
    "$$\n",
    "\\nabla E(u, v) = \\left[\\begin{array}{l}\n",
    "\\frac{\\partial E}{\\partial u} \\\\\n",
    "\\frac{\\partial E}{\\partial v}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{l}\n",
    "e^{u}+v e^{u v}+2 u-2 v-3 \\\\\n",
    "2 e^{2 v}+u e^{u v}-2 u+4 v-2\n",
    "\\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\nabla E(0, 0) = \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "\n",
    "In class, we have taught that the update rule of the gradient descent algorithm is \n",
    "\n",
    "$$\n",
    "\\left(u_{t+1}, v_{t+1}\\right)=\\left(u_{t}, v_{t}\\right)-\\eta \\nabla E\\left(u_{t}, v_{t}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please start from $\\left(u_{0}, v_{0}\\right)=(0,0)$, and fix $\\eta = 0.01$, what is $E\\left(u_{5}, v_{5}\\right)$ after five updates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "E = lambda u, v: np.exp(u) + np.exp(2 * v) + np.exp(u * v) + \\\n",
    "                    u ** 2 - 2 * u * v + 2 * v ** 2 - 3 * u - 2 * v\n",
    "\n",
    "grad_E = lambda u, v: np.array([np.exp(u) + v * np.exp(u * v) + 2 * u - 2 * v - 3,\n",
    "                                2 * np.exp(2 * v) + u * np.exp(u * v) - 2 * u + 4 * v - 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8250003566832635"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.01\n",
    "ut, vt = 0, 0\n",
    "\n",
    "for i in range(5):\n",
    "    ut, vt = (ut, vt) - eta * grad_E(ut, vt)\n",
    "\n",
    "# After 5 updates    \n",
    "E(ut, vt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $E\\left(u_{5}, v_{5}\\right)$ is about 2.825."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "Continue from Question 7, if we approximate the $E(u+\\Delta u, v+\\Delta v)$ by $\\hat{E}_{2}(\\Delta u, \\Delta v)$, where $\\hat{E}_{2}$ is the second-order Taylor's expansion of $E$ around $(u, v)$. Suppose \n",
    "\n",
    "$$\n",
    "\\hat{E}_{2}(\\Delta u, \\Delta v)=b_{u u}(\\Delta u)^{2}+b_{v v}(\\Delta v)^{2}+b_{u v}(\\Delta u)(\\Delta v)+b_{u} \\Delta u+b_{v} \\Delta v+b\n",
    "$$\n",
    "\n",
    "What are the values of $\\left(b_{u u}, b_{v v}, b_{u v}, b_{u}, b_{v}, b\\right)$ around $(u, v) = (0, 0)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "The Hessian matrix of $E$ is \n",
    "\n",
    "$$\n",
    "H_{E}(u, v)=\\left[\\begin{array}{cc}\n",
    "\\frac{\\partial^{2} E}{\\partial u^{2}} & \\frac{\\partial^{2} E}{\\partial u \\partial v} \\\\\n",
    "\\frac{\\partial^{2} E}{\\partial v \\partial u} & \\frac{\\partial^{2} E}{\\partial v^{2}}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{ll}\n",
    "e^{u}+v^{2} e^{u v}+2 & e^{u v}+u v e^{u v}-2 \\\\\n",
    "e^{u v}+u v e^{u v}-2 & 4 e^{2v}+u^{2} e^{u v}+4\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "\n",
    "$$\n",
    "H_{E}(0,0)=\\left[\\begin{array}{cc}\n",
    "3 & -1 \\\\\n",
    "-1 & 8\n",
    "\\end{array}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote $\\mathbf{h} = (\\Delta u, \\Delta v)^T$, then the second-order Taylor's expansion of $E$ around $(0, 0)$ can also be written as\n",
    "\n",
    "$$\n",
    "\\hat{E}_{2}(\\Delta u, \\Delta v) = \\hat{E}_2 (\\mathbf{h}) = E(0, 0) + \\nabla E (0, 0) \\cdot \\mathbf{h} + \\frac{1}{2} \\mathbf{h}^T H_E(0, 0) \\mathbf{h}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $b_{uu} = 3/2, b_{v v} = 4, b_{u v} = -1$. By Question 6, $b_{u} = -2, b_{v} = 0$, and $b = E(0, 0) = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**\n",
    "\n",
    "Continue from Question 8 and denote the Hessian matrix be $\\nabla^{2} E(u, v)$, and assume that the Hessian matrix is positive definite. What is the optimal $(\\Delta u, \\Delta v)$ to minimize $\\hat{E}_{2}(\\Delta u, \\Delta v)$? The direction is called the Newton Direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "The second-order Taylor's expansion of $E$ around $(u, v)$ can be written as\n",
    "\n",
    "$$\n",
    "\\hat{E}_{2}(\\Delta u, \\Delta v) = \\hat{E}_2 (\\mathbf{h}) = E(u, v) + \\nabla E (u, v) \\cdot \\mathbf{h} + \\frac{1}{2} \\mathbf{h}^T H_E(u, v) \\mathbf{h}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take derivative with respect to $\\Delta u$ and $\\Delta v$ to get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{E_2}}{\\partial \\Delta u} = \\frac{\\partial E}{\\partial u} +  \\frac{\\partial^2 E}{\\partial u^2} \\Delta u + \\frac{\\partial^2 E}{\\partial u \\partial v} \\Delta v \\\\\n",
    "\\frac{\\partial \\hat{E_2}}{\\partial \\Delta v} = \\frac{\\partial E}{\\partial v} +  \\frac{\\partial^2 E}{\\partial v^2} \\Delta v + \\frac{\\partial^2 E}{\\partial u \\partial v} \\Delta u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set them to equal 0, we obtain\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = (\\Delta u, \\Delta v)^T = -(\\nabla^{2} E(u, v))^{-1} \\nabla E (u, v).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**\n",
    "\n",
    "Using the Newton direction (without $\\eta$) to update, please start from $\\left(u_{0}, v_{0}\\right)=(0,0)$, what is $E(u_5, v_5)$ after five updates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hessian matrix\n",
    "H_E = lambda u, v: np.array([[np.exp(u) + v ** 2 * np.exp(u * v) + 2, np.exp(u * v) + u * v * np.exp(u * v) - 2],\n",
    "                             [np.exp(u * v) + u * v * np.exp(u * v) - 2, 4 * np.exp(2 * v) + u ** 2 * np.exp(u * v) + 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.360823345643139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut, vt = 0, 0\n",
    "\n",
    "for i in range(5):\n",
    "    newton_dir = -inv(H_E(ut, vt)) @ grad_E(ut, vt)\n",
    "    ut, vt = (ut, vt) + newton_dir\n",
    "\n",
    "# After 5 updates    \n",
    "E(ut, vt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the value of $E$ drops more quickly using Newton's method than gradient descent in this example. But this gain is at the cost of computing Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**\n",
    "\n",
    "For Question 11-12, you will play with feature transforms\n",
    "\n",
    "Consider six inputs $\\mathbf{x}_{1}=(1,1), \\mathbf{x}_{2}=(1,-1), \\mathbf{x}_{3}=(-1,-1), \\mathbf{x}_{4}=(-1,1), \\mathbf{x}_{5}=(0,0), \\mathbf{x}_{6}=(1,0)$. What is the biggest subset of those input vectors that can be shattered by the union of quadratic, linear, or constant hypothesis of $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol::**\n",
    "\n",
    "Consider the general quadratic transform of $\\mathbf{x}=(x_1, x_2)$ by\n",
    "$$\n",
    "f(\\mathbf{x}) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, $f$ takes inputs $\\mathbf{x_1}, \\cdots, \\mathbf{x_6}$ to $\\mathbf{z}_1 = f(\\mathbf{x_1}), \\cdots, \\mathbf{z}_2 = f(\\mathbf{x_6})$, which is shown as the following matrix:\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "1 &1 &1 &1 &1 &1 \\\\\n",
    "1 &1& -1& 1& 1& -1 \\\\\n",
    "1 & -1 & -1 & 1 & 1 & 1 \\\\\n",
    "1 & -1 & 1 &1 & 1 & -1 \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix $Z$ has full rank, which means for any $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_{5+1} \\end{bmatrix}$, the equation\n",
    "\n",
    "$$\n",
    "Z\\mathbf{\\tilde{w}} = \\mathbf{y}\n",
    "$$\n",
    "\n",
    "has a solution $\\mathbf{\\tilde{w}} = Z^{-1} \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, these six points $\\mathbf{z}_1 \\cdots \\mathbf{z}_6$ can be shattered by lines in the transformed space. However, lines in the transformed space correspond to exactly quadratic curves in the original space, and hence, $\\mathbf{x}_1, \\cdots, \\mathbf{x}_6$ can be shattered by the union of quadratic, linear, or constant hypothesis of $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**\n",
    "\n",
    "Assume that a transformer peeks the data and decides the following transform $\\mathbf{\\Phi}$ \"intelligently\" from the data of size $N$. The transform maps $\\mathbf{x} \\in \\mathbb{R}^d$ to $\\mathbf{z} \\in \\mathbb{R}^N$ , where\n",
    "\n",
    "$$\n",
    "(\\Phi(\\mathbf{x}))_{n}=z_{n}=\\left[\\mathbf{x}=\\mathbf{x}_{n}\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a learning algorithm that performs linear classification after the feature transform. That is, the algorithm effectively works on an $\\mathcal{H}_{\\mathbf{\\Phi}}$ that includes all possible $\\mathbf{\\Phi}$. What is $d_{v c}\\left(\\mathcal{H}_{\\Phi}\\right)$ (i.e., the maximum number of points that can be shattered by the process above)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "If we apply $\\mathbf{\\Phi}$ to the training data $\\mathbf{x}_1, \\cdots, \\mathbf{x}_N$, then\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi}(\\mathbf{x}_n) = \\mathbf{e}_n \\quad \\forall n=1, \\cdots N,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{e}_n$ is the $n$-th standard basis vector in $\\mathbb{R}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put these $N$ vectors in the matrix $Z$ row by row, then we can find that $Z$ is exactly the identity matrix $I_{N\\times N}$. So by the same argument as Question 11, the training data $\\mathbf{x}_1, \\cdots, \\mathbf{x}_N$ can be shattered by $\\mathcal{H}_{\\mathbf{\\Phi}}$. This is true for all $N$, so the VC dimension of this hypothesis set is $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13**\n",
    "\n",
    "For Question 13-15, you will play with linear regression and feature transforms. Consider the target function:\n",
    "\n",
    "$$\n",
    "f\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(x_{1}^{2}+x_{2}^{2}-0.6\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a training set of $N = 1000$ points on $\\mathcal{X}=[-1,1] \\times[-1,1]$ with uniform probablity of picking each $\\mathbf{x} \\in \\mathcal{X}$. Generate simulated noise by flipping the sign of the output in a random 10% subset of the generated training set.\n",
    "\n",
    "Carry out linear regression without transformation, i.e., with feature vector: $(1, x_1, x_2)$, to find the weight $\\mathbf{w}$, and use $\\mathbf{w}_{\\text{lin}}$ directly for classification. What is the closest value to the classification (0/1) in-sample error ($E_{\\text{in}}$)? Run the experiment 1000 times and take the average $E_{\\text{in}}$ in order to reduce variation in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target function\n",
    "# def f(x1, x2):\n",
    "#     def sign(x):\n",
    "#         return 1 if x > 0 else -1\n",
    "    \n",
    "#     return sign(x1 ** 2 + x2 ** 2 - 0.6)\n",
    "\n",
    "def generate_data(N):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        N: int\n",
    "    Returns:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ]\n",
    "    \"\"\"\n",
    "    X = np.random.uniform(low=-1.0, high=1.0, size=(N, 2))\n",
    "    y = np.zeros(N)\n",
    "    \n",
    "    y = np.sign(X[:, 0] ** 2 + X[:, 1] ** 2 - 0.6)\n",
    "#     # Below does not take advantage of the numpy vectorized operation and is slow\n",
    "#     i = 0\n",
    "#     for x in X:\n",
    "#         y[i] = f(*x)\n",
    "#         i += 1\n",
    "    \n",
    "    # Add noise to y\n",
    "    p = 0.1\n",
    "    offset = int(N * p)\n",
    "    random_index = np.random.permutation(N)\n",
    "    \n",
    "    noisy_y_index = random_index[:offset]\n",
    "    y[noisy_y_index] *= -1\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "X, y = generate_data(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(X[y==1][:, 0], X[y==1][:, 1], color='blue', marker='o', linestyle='None', label='+1')\n",
    "ax.plot(X[y==-1][:, 0], X[y==-1][:, 1], color='red', marker='x', linestyle='None', label='-1'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(X, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ]    \n",
    "    Returns:\n",
    "        w: ndarray of shape = [d, ]\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    X = np.c_[np.ones((N, 1)), X]\n",
    "    w = pinv(X.T @ X) @ X.T @ y\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06586025,  0.02371489, -0.01288701])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lin = lin(X, y)\n",
    "w_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_err(X, y, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ] \n",
    "        w: ndarray of shape = [d, ]\n",
    "    Returns:\n",
    "        err: float\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    X = np.c_[np.ones((N, 1)), X]\n",
    "    \n",
    "    y_hat = np.sign(X @ w)\n",
    "    err = np.mean(y_hat != y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.467"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_err(X, y, w_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_lin_no_trans(N):\n",
    "    err_list = []\n",
    "    for _ in range(1000):    \n",
    "        X, y = generate_data(N)    \n",
    "        w_lin = lin(X, y)\n",
    "        err = calc_err(X, y, w_lin)\n",
    "        err_list.append(err)\n",
    "    avg_err = np.mean(err_list)\n",
    "    return avg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508207"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_lin_no_trans(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14**\n",
    "Now, transform the training data into the following nonlinear feature vector:\n",
    "   \n",
    "$$\n",
    "\\left(1, x_{1}, x_{2}, x_{1} x_{2}, x_{1}^{2}, x_{2}^{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the vector $\\tilde{\\mathbf{w}}$ that corresponds to the solution of linear regression, and take it for classification.\n",
    "\n",
    "Which of the following hypotheses is closest to the one you find using linear regression on the transformed input? Closest here means agrees the most with your hypothesis (has the most probability of agreeing on a randomly selected point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-1.5 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+0.05 x_{1}^{2}+1.5 x_{2}^{2}\\right)\\\\\n",
    "&g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-1.5 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+0.05 x_{1}^{2}+0.05 x_{2}^{2}\\right)\\\\\n",
    "&g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-0.05 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+1.5 x_{1}^{2}+1.5 x_{2}^{2}\\right)\\\\\n",
    "&g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-0.05 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+1.5 x_{1}^{2}+15 x_{2}^{2}\\right)\\\\\n",
    "&g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-0.05 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+15 x_{1}^{2}+1.5 x_{2}^{2}\\right)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quadratic(x1, x2):\n",
    "#     return np.array([x1, x2, x1 * x2, x1 ** 2, x2 ** 2])\n",
    "\n",
    "def transform_X(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "    Returns:\n",
    "        Z: ndarray of shape = [N, d]\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    d = 5\n",
    "    Z = np.zeros((N, d))\n",
    "    Z[:, :2] = X[:]\n",
    "    Z[:, 2] = X[:, 0] * X[:, 1]\n",
    "    Z[:, 3] = X[:, 0] ** 2\n",
    "    Z[:, 4] = X[:, 1] ** 2\n",
    "    \n",
    "    # Below does not take advantage of the numpy vectorized operation and is slow\n",
    "#     i = 0\n",
    "#     for x in X:\n",
    "#         Z[i] = quadratic(*x)\n",
    "#         i += 1    \n",
    "    return Z\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9914943 , -0.07906799,  0.1171203 , -0.02982396,  1.53896951,\n",
       "        1.62686342])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = generate_data(N)  \n",
    "Z = transform_X(X)\n",
    "w_lin = lin(Z, y)\n",
    "w_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the closest hypothesis in the given hypotheses is\n",
    "\n",
    "$$\n",
    "g\\left(x_{1}, x_{2}\\right)=\\operatorname{sign}\\left(-1-0.05 x_{1}+0.08 x_{2}+0.13 x_{1} x_{2}+1.5 x_{1}^{2}+1.5 x_{2}^{2}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the simulation with the transformed data to see the classification error drops dramatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_lin_with_trans(N):\n",
    "    err_list = []\n",
    "    for _ in range(1000):    \n",
    "        X, y = generate_data(N)  \n",
    "        Z = transform_X(X)\n",
    "        w_lin = lin(Z, y)\n",
    "        err = calc_err(Z, y, w_lin)\n",
    "        err_list.append(err)\n",
    "    avg_err = np.mean(err_list)\n",
    "    return avg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.124313"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_lin_with_trans(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 15**\n",
    "\n",
    "What is the closest value to the classification out-of-sample error $E_{\\text{out}}$ of your hypothesis? Estimate it by generating a new set of 1000 points and adding noise as before. Average over 1000 runs to reduce the variation in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lin(N):\n",
    "    err_list = []\n",
    "    for _ in range(1000):    \n",
    "        X, y = generate_data(N)  \n",
    "        Z = transform_X(X)\n",
    "        w_lin = lin(Z, y)\n",
    "        \n",
    "        X_test, y_test = generate_data(N)\n",
    "        Z_test = transform_X(X_test)\n",
    "        # Calculate E_out\n",
    "        err = calc_err(Z_test, y_test, w_lin)\n",
    "        err_list.append(err)\n",
    "    avg_err = np.mean(err_list)\n",
    "    return avg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.126341"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lin(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 16**\n",
    "\n",
    "For Question 16-17, you will derive an algorithm for multinomial (multiclass) logistic regression. For a $K$-class classification problem, we will denote the output space $\\mathcal{Y}=\\{1,2, \\cdots, K\\}$. The hypotheses considered by MLR are indexed by a list of weight vectors $\\left(\\mathbf{w}_{1}, \\cdots, \\mathbf{w}_{K}\\right)$, each weight vector of length $d+1$. Each list represents a hypothesis\n",
    "\n",
    "$$\n",
    "h_{y}(\\mathbf{x})=\\frac{\\exp \\left(\\mathbf{w}_{y}^{T} \\mathbf{x}\\right)}{\\sum_{i=1}^{K} \\exp \\left(\\mathbf{w}_{i}^{T} \\mathbf{x}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that can be used to approximate the target distribution $P(y | \\mathbf{x})$. MLR then seeks for the maximum likelihood solution over all such hypotheses.\n",
    "\n",
    "For general $K$, derive an $E_{\\mathrm{in}}\\left(\\mathbf{w}_{1}, \\cdots, \\mathbf{w}_{K}\\right)$ like page 11 of Lecture 10 slides by minimizing the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "The likelihood function of multinomial logistic hypothesis is \n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}_1, \\cdots, \\mathbf{w}_K) = \\prod_{n=1}^{N} f(\\mathbf{x}_n, y_n) = \\prod_{n=1}^{N} P(y_n | \\mathbf{x}_n) P(\\mathbf{x}_n) = \\prod_{n=1}^{N} h_{y_n}( \\mathbf{x}_n) P(\\mathbf{x}_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the log likelihood is\n",
    "\n",
    "$$\n",
    "\\log L(\\mathbf{w}_1, \\cdots, \\mathbf{w}_K) = \\sum_{n=1}^N \\left[\\log h_{y_n}( \\mathbf{x}_n) + \\log P(\\mathbf{x}_n)\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and maximizing log likelihood is equivalent to minimize its negative, i.e.,\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K}  \\log L \\iff \\min_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K} -\\sum_{n=1}^N \\left[\\log h_{y_n}( \\mathbf{x}_n) + \\log P(\\mathbf{x}_n)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the second term $\\log P(\\mathbf{x}_n)$ does not depend on $\\mathbf{w}_1, \\cdots, \\mathbf{w}_K$, we can simply ignore it. Then \n",
    "\n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K}  \\log L &\\iff\n",
    "\\min_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K} -\\sum_{n=1}^N \\log h_{y_n}( \\mathbf{x}_n) \\\\\n",
    "&\\iff \\min_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K} \\sum_{n=1}^N \\left[ \\log\\left(\\sum_{i=1}^{K} \\exp \\left(\\mathbf{w}_{i}^{T} \\mathbf{x}_n \\right) \\right) -\\mathbf{w}^T_{y_n} \\mathbf{x}_n \\right]  \\\\\n",
    "&\\iff \\min_{\\mathbf{w}_1, \\cdots, \\mathbf{w}_K} \\frac{1}{N} \\sum_{n=1}^N \\left[ \\log\\left(\\sum_{i=1}^{K} \\exp \\left(\\mathbf{w}_{i}^{T} \\mathbf{x}_n \\right) \\right) -\\mathbf{w}^T_{y_n} \\mathbf{x}_n \\right] \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the error function we want to minimize is \n",
    "\n",
    "$$\n",
    "E_{\\mathrm{in}}\\left(\\mathbf{w}_{1}, \\cdots, \\mathbf{w}_{K}\\right) = \\frac{1}{N} \\sum_{n=1}^N \\left[ \\log\\left(\\sum_{i=1}^{K} \\exp \\left(\\mathbf{w}_{i}^{T} \\mathbf{x}_n \\right) \\right) -\\mathbf{w}^T_{y_n} \\mathbf{x}_n \\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 17**\n",
    "\n",
    "For the $E_{\\text{in}}$ derived above, its gradient $\\nabla E_{\\mathrm{in}}$ can be represented by $\\left(\\frac{\\partial E_{\\text {in }}}{\\partial \\mathbf{w}_{1}}, \\frac{\\partial E_{\\text {in }}}{\\partial \\mathbf{w}_{2}}, \\cdots, \\frac{\\partial E_{\\text {in }}}{\\partial \\mathbf{w}_K}\\right)$, write down $\\frac{\\partial E_{\\text {in }}}{\\partial \\mathbf{w}_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:**\n",
    "\n",
    "Take the derivative with respect to $\\mathbf{w}_k$, we can obtain\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E_{\\text {in }}}{\\partial \\mathbf{w}_k} &= \\frac{1}{N} \\sum_{n=1}^N \\left[\\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_n) \\mathbf{x}_n}{\\sum_{i=1}^{K} \\exp \\left(\\mathbf{w}_{i}^{T} \\mathbf{x}_n \\right)} - [y_n = k] \\mathbf{x}_n  \\right] \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\left[\\left(h_{k}(\\mathbf{x}_n) - [y_n = k]\\right) \\mathbf{x}_n \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 18**\n",
    "\n",
    "For Question 18-20, you will play with logistic regression.\n",
    "\n",
    "Please use the following set `hw3_train.dat` for training and the following set `hw3_test.dat` for testing.\n",
    "\n",
    "Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\\eta = 0.001$ and $T = 2000$. What is $E_{\\text{out}}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_err(X, y, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d + 1]\n",
    "        y: ndarray of shape = [N, 1]\n",
    "        w: ndarray of shape = [d + 1, 1]\n",
    "    returns:\n",
    "        grad: ndarray of shape = [d + 1, 1]\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    grad = (X * (-y)).T @ sigmoid(X @ w * (-y)) / N\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(X, y, eta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ]\n",
    "        eta: float\n",
    "    Returns:\n",
    "        wt: ndarray of shape = [d + 1, ]\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    X = np.c_[np.ones((N, 1)), X]\n",
    "#     wt = np.random.normal(size=(d+1, 1))  # initialize using normal distribution requires more steps to converge in this example\n",
    "    wt = np.zeros((d + 1, 1))               # initialize all parameters to zero converges more quickly\n",
    "    y = y.reshape(-1, 1)                    # reshape y to a two dimensional ndarray to make `calc_grad_err` function easy to implement\n",
    "     \n",
    "    T = 2000\n",
    "    for t in range(T):\n",
    "        grad_t = calc_grad_err(X, y, wt)\n",
    "        wt -= eta * grad_t\n",
    "\n",
    "    wt = wt.flatten()\n",
    "    return wt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_one_err(X, y, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ]\n",
    "        w: ndarray of shape = [d + 1, ]\n",
    "    Returns:\n",
    "        avg_err: float\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    X = np.c_[np.ones((N, 1)), X]\n",
    "    y_hat = np.sign(X @ w)  # h(x) > 1/2 <==> w^T x > 0\n",
    "    avg_err = np.mean(y_hat != y)\n",
    "    return avg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training and test data\n",
    "df_train = pd.read_csv('hw3_train.dat', header=None, sep='\\s+')\n",
    "X_train_df, y_train_df = df_train.iloc[:, :-1], df_train.iloc[:, -1]\n",
    "X_train, y_train = X_train_df.to_numpy(), y_train_df.to_numpy()\n",
    "\n",
    "df_test = pd.read_csv('hw3_test.dat', header=None, sep='\\s+')\n",
    "X_test_df, y_test_df = df_test.iloc[:, :-1], df_test.iloc[:, -1]\n",
    "X_test, y_test = X_test_df.to_numpy(), y_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "w_lr = lr(X_train, y_train, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.466"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_in\n",
    "calc_zero_one_err(X_train, y_train, w_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.475"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_out\n",
    "calc_zero_one_err(X_test, y_test, w_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 19**\n",
    "\n",
    "Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\\eta = 0.01$ and $T = 2000$, what is $E_{\\text{out}}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "w_lr = lr(X_train, y_train, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.197"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_in\n",
    "calc_zero_one_err(X_train, y_train, w_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_out\n",
    "calc_zero_one_err(X_test, y_test, w_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 20**\n",
    "\n",
    "Implement the fixed learning rate stochastic gradient descent algorithm for logistic regression. Instead of randomly choosing $n$ in each iteration, please simply pick the example with the cyclic order $n = 1, 2, \\cdots, N, 1, 2, \\cdots$\n",
    "\n",
    "Run the algorithm with $\\eta = 0.001$ and $T = 2000$, what is $E_{\\text{out}}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sgd(X, y, eta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of shape = [N, d]\n",
    "        y: ndarray of shape = [N, ]\n",
    "        eta: float\n",
    "    Returns:\n",
    "        wt: ndarray of shape = [d + 1, ]\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    X = np.c_[np.ones((N, 1)), X]\n",
    "    wt = np.zeros((d + 1, 1))               \n",
    "    y = y.reshape(-1, 1)                    \n",
    "     \n",
    "    T = 2000\n",
    "    n = 0\n",
    "    for t in range(T):\n",
    "        xt, yt = X[n].reshape(-1, 1).T, y[n]\n",
    "        grad_t = calc_grad_err(xt, yt, wt)\n",
    "        wt -= eta * grad_t\n",
    "        n = n + 1 if n != N - 1 else 0\n",
    "\n",
    "    wt = wt.flatten()\n",
    "    return wt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "w_lr = lr_sgd(X_train, y_train, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.464"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_in\n",
    "calc_zero_one_err(X_train, y_train, w_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.473"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E_out\n",
    "calc_zero_one_err(X_test, y_test, w_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this data set, we need more iterations for SGD to converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th_daily",
   "language": "python",
   "name": "th"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
